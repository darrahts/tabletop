{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process Example\n",
    "- 2 states, s1 and s2\n",
    "- 2 actions, a1 and a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from World import World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(i, j, k, K, n, t, v):\n",
    "    first = \"i: {}\\tj: {}\\tn: {}\\tT(k,j,n): {}\\tV(k): {}\".format(i, j, n, t, v)\n",
    "    others = \"T(k,j,n): {}\\tV(k): {}\".format(t, v)\n",
    "    if(k == 0):\n",
    "        print(first, end=\"\")\n",
    "    else:\n",
    "        print(others, end=\"\")\n",
    "    if(k != K):\n",
    "        print(\" +\\t\", end=\"\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "def value_iteration(states, actions, rewards, values, gamma, beta, verbose=False):\n",
    "    #rewards = [3, -1]\n",
    "    #values = rewards[:]\n",
    "    old_values = np.ones(len(rewards))*-999\n",
    "    #stopping_criteria = .005\n",
    "    policy = dict.fromkeys(states, \"\")\n",
    "    i = 0\n",
    "    while(abs(sum(values) - sum(old_values)) > beta):\n",
    "        old_values = values[:]\n",
    "        for j in range(0, len(states)): # all states in S\n",
    "            vals = np.zeros(len(actions))\n",
    "            for n in range(0, len(actions)): # valid actions in Sj\n",
    "                assert vals[n] == 0\n",
    "                for k in range(0, len(states)): # reachabe states from Sj\n",
    "                    vals[n] = vals[n] + T[k][j][n]*values[k]\n",
    "                    if(verbose):\n",
    "                        debug(i, j, k, len(states)-1, n, T[k][j][n], values[k])\n",
    "            values[j] = rewards[j] + gamma * np.amax(vals)\n",
    "            policy[states[j]] = actions[np.argmax(vals)]\n",
    "            if(verbose):\n",
    "                print(\"Vals: {}\".format(vals))\n",
    "                print(\"V({0}) = R({0}) + gamma*max({1})\".format(j, vals))\n",
    "                print(\"V({0}) = {1} + {2}*{3}\".format(j, rewards[j], gamma, np.amax(vals)))\n",
    "                print(\"V(\" + str(j) + \"): \" + str(values[j]))\n",
    "        i = i + 1\n",
    "    if(verbose):\n",
    "        print(\"Stopping criteria met. state values have been permanently assigned.\")\n",
    "        print(\"V(s1) = {:.2f}\\tV(s2) = {:.2f}\".format(values[0], values[1]))\n",
    "        print(\"R(s1) = {:.2f}\\tR(s2) = {:.2f}\".format(rewards[0], rewards[1]))\n",
    "        print(\"Optimal policy: \")\n",
    "        for key, value in policy.items():\n",
    "            print(\"in state (\" + key + \") take action (\" + value+ \")\")\n",
    "    return values, policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 3 +\tT(k,j,n): 1.0\tV(k): -1\n",
      "i: 0\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 3 +\tT(k,j,n): 0.5\tV(k): -1\n",
      "i: 0\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 3.5 +\tT(k,j,n): 0\tV(k): -1\n",
      "i: 0\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 3.5 +\tT(k,j,n): 1.0\tV(k): -1\n",
      "i: 1\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 3.5 +\tT(k,j,n): 1.0\tV(k): 0.75\n",
      "i: 1\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 3.5 +\tT(k,j,n): 0.5\tV(k): 0.75\n",
      "i: 1\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.0625 +\tT(k,j,n): 0\tV(k): 0.75\n",
      "i: 1\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.0625 +\tT(k,j,n): 1.0\tV(k): 0.75\n",
      "i: 2\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.0625 +\tT(k,j,n): 1.0\tV(k): 1.03125\n",
      "i: 2\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.0625 +\tT(k,j,n): 0.5\tV(k): 1.03125\n",
      "i: 2\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.2734375 +\tT(k,j,n): 0\tV(k): 1.03125\n",
      "i: 2\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.2734375 +\tT(k,j,n): 1.0\tV(k): 1.03125\n",
      "i: 3\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.2734375 +\tT(k,j,n): 1.0\tV(k): 1.13671875\n",
      "i: 3\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.2734375 +\tT(k,j,n): 0.5\tV(k): 1.13671875\n",
      "i: 3\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.3525390625 +\tT(k,j,n): 0\tV(k): 1.13671875\n",
      "i: 3\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.3525390625 +\tT(k,j,n): 1.0\tV(k): 1.13671875\n",
      "i: 4\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.3525390625 +\tT(k,j,n): 1.0\tV(k): 1.17626953125\n",
      "i: 4\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.3525390625 +\tT(k,j,n): 0.5\tV(k): 1.17626953125\n",
      "i: 4\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.3822021484375 +\tT(k,j,n): 0\tV(k): 1.17626953125\n",
      "i: 4\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.3822021484375 +\tT(k,j,n): 1.0\tV(k): 1.17626953125\n",
      "i: 5\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.3822021484375 +\tT(k,j,n): 1.0\tV(k): 1.19110107421875\n",
      "i: 5\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.3822021484375 +\tT(k,j,n): 0.5\tV(k): 1.19110107421875\n",
      "i: 5\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.3933258056640625 +\tT(k,j,n): 0\tV(k): 1.19110107421875\n",
      "i: 5\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.3933258056640625 +\tT(k,j,n): 1.0\tV(k): 1.19110107421875\n",
      "i: 6\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.3933258056640625 +\tT(k,j,n): 1.0\tV(k): 1.1966629028320312\n",
      "i: 6\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.3933258056640625 +\tT(k,j,n): 0.5\tV(k): 1.1966629028320312\n",
      "i: 6\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.397497177124023 +\tT(k,j,n): 0\tV(k): 1.1966629028320312\n",
      "i: 6\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.397497177124023 +\tT(k,j,n): 1.0\tV(k): 1.1966629028320312\n",
      "i: 7\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.397497177124023 +\tT(k,j,n): 1.0\tV(k): 1.1987485885620117\n",
      "i: 7\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.397497177124023 +\tT(k,j,n): 0.5\tV(k): 1.1987485885620117\n",
      "i: 7\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.399061441421509 +\tT(k,j,n): 0\tV(k): 1.1987485885620117\n",
      "i: 7\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.399061441421509 +\tT(k,j,n): 1.0\tV(k): 1.1987485885620117\n"
     ]
    }
   ],
   "source": [
    "states = [\"s1\", \"s2\"]\n",
    "actions = [\"a1\", \"a2\"]\n",
    "rewards = [3, -1]\n",
    "\n",
    "# initialize the values\n",
    "# possible choices are random, zeros, or set to reward value\n",
    "values = rewards[:]\n",
    "\n",
    "# T is a jxjxn matrix of transition probabilities P(Sj | Si, An) for all Sj, Si, and An\n",
    "# P(s1 | s1, a1) = 0.0\n",
    "# P(s1 | s1, a2) = 0.5 \n",
    "# .... \n",
    "# P(s2 | s2, a2) = 1.0\n",
    "# This information must be given (or learned, but that is a different problem)\n",
    "T = [[[0, .5], [1.0, 0]], [[1.0, .5], [0, 1.0]]]\n",
    "\n",
    "# the weight factor for how much we care about future rewards\n",
    "gamma = .5\n",
    "\n",
    "# the stopping criteria, i.e. if improvement is less than this value then stop\n",
    "beta = .005\n",
    "values, policy = value_iteration(states, actions, rewards[:], values, gamma, beta, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: 0\n",
      "i: 0\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 3 +\tT(k,j,n): 1.0\tV(k): -1\n",
      "i: 0\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 3 +\tT(k,j,n): 0.5\tV(k): -1\n",
      "Vals: [-1.  1.]\n",
      "V(0) = R(0) + gamma*max([-1.  1.])\n",
      "V(0) = 3 + 0.5*1.0\n",
      "V(0): 3.5\n",
      "current state: 1\n",
      "i: 0\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 3.5 +\tT(k,j,n): 0\tV(k): -1\n",
      "i: 0\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 3.5 +\tT(k,j,n): 1.0\tV(k): -1\n",
      "Vals: [ 3.5 -1. ]\n",
      "V(1) = R(1) + gamma*max([ 3.5 -1. ])\n",
      "V(1) = -1 + 0.5*3.5\n",
      "V(1): 0.75\n",
      "current state: 0\n",
      "i: 1\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 3.5 +\tT(k,j,n): 1.0\tV(k): 0.75\n",
      "i: 1\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 3.5 +\tT(k,j,n): 0.5\tV(k): 0.75\n",
      "Vals: [0.75  2.125]\n",
      "V(0) = R(0) + gamma*max([0.75  2.125])\n",
      "V(0) = 3 + 0.5*2.125\n",
      "V(0): 4.0625\n",
      "current state: 1\n",
      "i: 1\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.0625 +\tT(k,j,n): 0\tV(k): 0.75\n",
      "i: 1\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.0625 +\tT(k,j,n): 1.0\tV(k): 0.75\n",
      "Vals: [4.0625 0.75  ]\n",
      "V(1) = R(1) + gamma*max([4.0625 0.75  ])\n",
      "V(1) = -1 + 0.5*4.0625\n",
      "V(1): 1.03125\n",
      "current state: 0\n",
      "i: 2\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.0625 +\tT(k,j,n): 1.0\tV(k): 1.03125\n",
      "i: 2\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.0625 +\tT(k,j,n): 0.5\tV(k): 1.03125\n",
      "Vals: [1.03125  2.546875]\n",
      "V(0) = R(0) + gamma*max([1.03125  2.546875])\n",
      "V(0) = 3 + 0.5*2.546875\n",
      "V(0): 4.2734375\n",
      "current state: 1\n",
      "i: 2\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.2734375 +\tT(k,j,n): 0\tV(k): 1.03125\n",
      "i: 2\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.2734375 +\tT(k,j,n): 1.0\tV(k): 1.03125\n",
      "Vals: [4.2734375 1.03125  ]\n",
      "V(1) = R(1) + gamma*max([4.2734375 1.03125  ])\n",
      "V(1) = -1 + 0.5*4.2734375\n",
      "V(1): 1.13671875\n",
      "current state: 0\n",
      "i: 3\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.2734375 +\tT(k,j,n): 1.0\tV(k): 1.13671875\n",
      "i: 3\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.2734375 +\tT(k,j,n): 0.5\tV(k): 1.13671875\n",
      "Vals: [1.13671875 2.70507812]\n",
      "V(0) = R(0) + gamma*max([1.13671875 2.70507812])\n",
      "V(0) = 3 + 0.5*2.705078125\n",
      "V(0): 4.3525390625\n",
      "current state: 1\n",
      "i: 3\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.3525390625 +\tT(k,j,n): 0\tV(k): 1.13671875\n",
      "i: 3\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.3525390625 +\tT(k,j,n): 1.0\tV(k): 1.13671875\n",
      "Vals: [4.35253906 1.13671875]\n",
      "V(1) = R(1) + gamma*max([4.35253906 1.13671875])\n",
      "V(1) = -1 + 0.5*4.3525390625\n",
      "V(1): 1.17626953125\n",
      "current state: 0\n",
      "i: 4\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.3525390625 +\tT(k,j,n): 1.0\tV(k): 1.17626953125\n",
      "i: 4\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.3525390625 +\tT(k,j,n): 0.5\tV(k): 1.17626953125\n",
      "Vals: [1.17626953 2.7644043 ]\n",
      "V(0) = R(0) + gamma*max([1.17626953 2.7644043 ])\n",
      "V(0) = 3 + 0.5*2.764404296875\n",
      "V(0): 4.3822021484375\n",
      "current state: 1\n",
      "i: 4\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.3822021484375 +\tT(k,j,n): 0\tV(k): 1.17626953125\n",
      "i: 4\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.3822021484375 +\tT(k,j,n): 1.0\tV(k): 1.17626953125\n",
      "Vals: [4.38220215 1.17626953]\n",
      "V(1) = R(1) + gamma*max([4.38220215 1.17626953])\n",
      "V(1) = -1 + 0.5*4.3822021484375\n",
      "V(1): 1.19110107421875\n",
      "current state: 0\n",
      "i: 5\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.3822021484375 +\tT(k,j,n): 1.0\tV(k): 1.19110107421875\n",
      "i: 5\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.3822021484375 +\tT(k,j,n): 0.5\tV(k): 1.19110107421875\n",
      "Vals: [1.19110107 2.78665161]\n",
      "V(0) = R(0) + gamma*max([1.19110107 2.78665161])\n",
      "V(0) = 3 + 0.5*2.786651611328125\n",
      "V(0): 4.3933258056640625\n",
      "current state: 1\n",
      "i: 5\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.3933258056640625 +\tT(k,j,n): 0\tV(k): 1.19110107421875\n",
      "i: 5\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.3933258056640625 +\tT(k,j,n): 1.0\tV(k): 1.19110107421875\n",
      "Vals: [4.39332581 1.19110107]\n",
      "V(1) = R(1) + gamma*max([4.39332581 1.19110107])\n",
      "V(1) = -1 + 0.5*4.3933258056640625\n",
      "V(1): 1.1966629028320312\n",
      "current state: 0\n",
      "i: 6\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.3933258056640625 +\tT(k,j,n): 1.0\tV(k): 1.1966629028320312\n",
      "i: 6\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.3933258056640625 +\tT(k,j,n): 0.5\tV(k): 1.1966629028320312\n",
      "Vals: [1.1966629  2.79499435]\n",
      "V(0) = R(0) + gamma*max([1.1966629  2.79499435])\n",
      "V(0) = 3 + 0.5*2.794994354248047\n",
      "V(0): 4.397497177124023\n",
      "current state: 1\n",
      "i: 6\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.397497177124023 +\tT(k,j,n): 0\tV(k): 1.1966629028320312\n",
      "i: 6\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.397497177124023 +\tT(k,j,n): 1.0\tV(k): 1.1966629028320312\n",
      "Vals: [4.39749718 1.1966629 ]\n",
      "V(1) = R(1) + gamma*max([4.39749718 1.1966629 ])\n",
      "V(1) = -1 + 0.5*4.397497177124023\n",
      "V(1): 1.1987485885620117\n",
      "current state: 0\n",
      "i: 7\tj: 0\tn: 0\tT(k,j,n): 0\tV(k): 4.397497177124023 +\tT(k,j,n): 1.0\tV(k): 1.1987485885620117\n",
      "i: 7\tj: 0\tn: 1\tT(k,j,n): 0.5\tV(k): 4.397497177124023 +\tT(k,j,n): 0.5\tV(k): 1.1987485885620117\n",
      "Vals: [1.19874859 2.79812288]\n",
      "V(0) = R(0) + gamma*max([1.19874859 2.79812288])\n",
      "V(0) = 3 + 0.5*2.7981228828430176\n",
      "V(0): 4.399061441421509\n",
      "current state: 1\n",
      "i: 7\tj: 1\tn: 0\tT(k,j,n): 1.0\tV(k): 4.399061441421509 +\tT(k,j,n): 0\tV(k): 1.1987485885620117\n",
      "i: 7\tj: 1\tn: 1\tT(k,j,n): 0\tV(k): 4.399061441421509 +\tT(k,j,n): 1.0\tV(k): 1.1987485885620117\n",
      "Vals: [4.39906144 1.19874859]\n",
      "V(1) = R(1) + gamma*max([4.39906144 1.19874859])\n",
      "V(1) = -1 + 0.5*4.399061441421509\n",
      "V(1): 1.1995307207107544\n",
      "Stopping criteria met. state values have been permanently assigned.\n",
      "V(s1) = 4.40\tV(s2) = 1.20\n",
      "R(s1) = 3.00\tR(s2) = -1.00\n",
      "Optimal policy: \n",
      "in state (s1) take action (a2)\n",
      "in state (s2) take action (a1)\n"
     ]
    }
   ],
   "source": [
    "rewards = [3, -1]\n",
    "values = rewards[:]\n",
    "old_values = np.ones(len(rewards))*-999\n",
    "stopping_criteria = .005\n",
    "policy = dict.fromkeys(states, \"\")\n",
    "i = 0\n",
    "while(abs(sum(values) - sum(old_values)) > stopping_criteria):\n",
    "    old_values = values[:]\n",
    "    for j in range(0, len(states)): # all states in S\n",
    "        vals = np.zeros(len(actions))\n",
    "        print(\"current state: \" + str(j))\n",
    "        for n in range(0, len(actions)): # valid actions in Sj\n",
    "            assert vals[n] == 0\n",
    "            for k in range(0, len(states)): # reachabe states from Sj\n",
    "                vals[n] = vals[n] + T[k][j][n]*values[k]\n",
    "                debug(i, j, k, len(states)-1, n, T[k][j][n], values[k])\n",
    "        print(\"Vals: {}\".format(vals))\n",
    "        values[j] = rewards[j] + gamma * np.amax(vals)\n",
    "        policy[states[j]] = actions[np.argmax(vals)]\n",
    "        print(\"V({0}) = R({0}) + gamma*max({1})\".format(j, vals))\n",
    "        print(\"V({0}) = {1} + {2}*{3}\".format(j, rewards[j], gamma, np.amax(vals)))\n",
    "        print(\"V(\" + str(j) + \"): \" + str(values[j]))\n",
    "    i = i + 1\n",
    "print(\"Stopping criteria met. state values have been permanently assigned.\")\n",
    "print(\"V(s1) = {:.2f}\\tV(s2) = {:.2f}\".format(values[0], values[1]))\n",
    "print(\"R(s1) = {:.2f}\\tR(s2) = {:.2f}\".format(rewards[0], rewards[1]))\n",
    "print(\"Optimal policy: \")\n",
    "for key, value in policy.items():\n",
    "    print(\"in state (\" + key + \") take action (\" + value+ \")\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.zeros(2)\n",
    "assert vals[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a2'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy[states[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vals)\n",
    "print(np.amax(vals))\n",
    "print(np.argmax(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a2'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[int(best_actions[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.399061441421509, 1.1995307207107544]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_values = values[:]\n",
    "old_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nRows': 3, 'nCols': 4, 'stateObstacles': [5], 'stateTerminals': [10, 11], 'nStates': 12, 'nActions': 4}\n",
      "Help on World in module World object:\n",
      "\n",
      "class World(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_nactions(self)\n",
      " |  \n",
      " |  get_ncols(self)\n",
      " |  \n",
      " |  get_nrows(self)\n",
      " |  \n",
      " |  get_nstates(self)\n",
      " |  \n",
      " |  get_stateobstacles(self)\n",
      " |  \n",
      " |  get_stateterminals(self)\n",
      " |  \n",
      " |  plot(self)\n",
      " |      plot function\n",
      " |      :return: None\n",
      " |  \n",
      " |  plot_policy(self, policy)\n",
      " |  \n",
      " |  plot_value(self, valueFunction)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "world = World()\n",
    "print(vars(world))\n",
    "print(help(world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
