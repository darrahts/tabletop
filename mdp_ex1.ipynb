{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process Example\n",
    "- 2 states, s1 and s2\n",
    "- 2 actions, a1 and a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from World import World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(i, j, k, K, n, t, v):\n",
    "    first = \"i: {}\\tj: {}\\tn: {}\\tT(k,j,n): {}\\tV(k): {}\".format(i, j, n, t, v)\n",
    "    others = \"T(k,j,n): {}\\tV(k): {}\".format(t, v)\n",
    "    if(k == 0):\n",
    "        print(first, end=\"\")\n",
    "    else:\n",
    "        print(others, end=\"\")\n",
    "    if(k != K):\n",
    "        print(\" +\\t\", end=\"\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "def value_iteration(states, actions, T, rewards, gamma, beta, values=[], verbose=False):\n",
    "    \"\"\"\n",
    "        @brief: implements Bellman's equation to assign the max expected state values\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is j), type should be string if verbose=True i.e. [\"s1\", \"s2\", \"living room\" ... \"etc\"]\n",
    "            actions: a list of actions (length is n), type should be string if verbose=True i.e. [\"a1\", \"a2\", \"take picture\" ... \"etc\"]\n",
    "            T: a jxjxn matrix of transition probabilities P(Sj | Si, An) for all Sj, Si, and An, j=i is valid (self transitions)\n",
    "            rewards: an array of length j specifying the reward for each state. The reward value never changes.\n",
    "            gamma: the discount factor, i.e. how much do we care about future rewards\n",
    "            beta: the stopping criteria, i.e. minimum amount of improvement between iterations before stopping\n",
    "            values: [optional] current values of the states, set to the states reward value if not supplied\n",
    "            verbose: [optional] set to false \n",
    "        \n",
    "        @output:\n",
    "            values: an array of state values where V(S(i)) = V[i]\n",
    "            policy: a dictionary mapping states as keys to the best action as values, i.e. policy[\"some state\"] -> \"some action\"\n",
    "    \"\"\"\n",
    "    if(len(values) == 0):\n",
    "        values = rewards[:]\n",
    "    old_values = np.ones(len(rewards))*-999\n",
    "    #stopping_criteria = .005\n",
    "    policy = dict.fromkeys(states, \"\")\n",
    "    i = 0\n",
    "    while(abs(sum(values) - sum(old_values)) > beta):\n",
    "        old_values = values[:]\n",
    "        for j in range(0, len(states)): # all states in S\n",
    "            vals = np.zeros(len(actions))\n",
    "            for n in range(0, len(actions)): # valid actions in Sj\n",
    "                assert vals[n] == 0\n",
    "                for k in range(0, len(states)): # reachabe states from Sj\n",
    "                    vals[n] = vals[n] + T[k][j][n]*values[k]\n",
    "                    if(verbose):\n",
    "                        debug(i, j, k, len(states)-1, n, T[k][j][n], values[k])\n",
    "            values[j] = rewards[j] + gamma * np.amax(vals)\n",
    "            policy[states[j]] = actions[np.argmax(vals)]\n",
    "            if(verbose):\n",
    "                print(\"Vals: {}\".format(vals))\n",
    "                print(\"V({0}) = R({0}) + gamma*max({1})\".format(j, vals))\n",
    "                print(\"V({0}) = {1} + {2}*{3}\".format(j, rewards[j], gamma, np.amax(vals)))\n",
    "                print(\"V(\" + str(j) + \"): \" + str(values[j]))\n",
    "        i = i + 1\n",
    "    print(\"Stopping criteria met. state values have been permanently assigned.\")\n",
    "    print(\"V(s1) = {:.2f}\\tV(s2) = {:.2f}\".format(values[0], values[1]))\n",
    "    print(\"R(s1) = {:.2f}\\tR(s2) = {:.2f}\".format(rewards[0], rewards[1]))\n",
    "    print(\"Optimal policy: \")\n",
    "    for key, value in policy.items():\n",
    "        print(\"in state (\" + key + \") take action (\" + value+ \")\")\n",
    "    return values, policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria met. state values have been permanently assigned.\n",
      "V(s1) = 4.40\tV(s2) = 1.20\n",
      "R(s1) = 3.00\tR(s2) = -1.00\n",
      "Optimal policy: \n",
      "in state (s1) take action (a2)\n",
      "in state (s2) take action (a1)\n",
      "[4.399061441421509, 1.1995307207107544]\n",
      "{'s1': 'a2', 's2': 'a1'}\n"
     ]
    }
   ],
   "source": [
    "states = [\"s1\", \"s2\"]\n",
    "actions = [\"a1\", \"a2\"]\n",
    "rewards = [3, -1]\n",
    "\n",
    "# initialize the values\n",
    "# possible choices are random, zeros, or set to reward value\n",
    "values = rewards[:]\n",
    "\n",
    "# T is a jxjxn matrix of transition probabilities P(Sj | Si, An) for all Sj, Si, and An\n",
    "# P(s1 | s1, a1) = 0.0\n",
    "# P(s1 | s1, a2) = 0.5 \n",
    "# .... \n",
    "# P(s2 | s2, a2) = 1.0\n",
    "# This information must be given (or learned, but that is a different problem)\n",
    "T = [[[0, .5], [1.0, 0]], [[1.0, .5], [0, 1.0]]]\n",
    "\n",
    "# the weight factor for how much we care about future rewards\n",
    "gamma = .5\n",
    "\n",
    "# the stopping criteria, i.e. if improvement is less than this value then stop\n",
    "beta = .005\n",
    "values, policy = value_iteration(states, actions, T, rewards[:], gamma, beta, values=rewards[:], verbose=False)\n",
    "print(values)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nRows': 3, 'nCols': 4, 'stateObstacles': [5], 'stateTerminals': [10, 11], 'nStates': 12, 'nActions': 4}\n",
      "Help on World in module World object:\n",
      "\n",
      "class World(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_nactions(self)\n",
      " |  \n",
      " |  get_ncols(self)\n",
      " |  \n",
      " |  get_nrows(self)\n",
      " |  \n",
      " |  get_nstates(self)\n",
      " |  \n",
      " |  get_stateobstacles(self)\n",
      " |  \n",
      " |  get_stateterminals(self)\n",
      " |  \n",
      " |  plot(self)\n",
      " |      plot function\n",
      " |      :return: None\n",
      " |  \n",
      " |  plot_policy(self, policy)\n",
      " |  \n",
      " |  plot_value(self, valueFunction)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "world = World()\n",
    "print(vars(world))\n",
    "print(help(world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
